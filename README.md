ğŸ§  Flask API â€“ PredicciÃ³n de CÃ¡ncer de Mama
1. ğŸš€ IntroducciÃ³n

Este proyecto implementa una API Flask que utiliza un modelo de clasificaciÃ³n para predecir el diagnÃ³stico de cÃ¡ncer de mama a partir del dataset Breast Cancer Wisconsin disponible en scikit-learn.

El sistema fue desarrollado como parte de la evaluaciÃ³n modular del MÃ³dulo de Machine Learning, enfocÃ¡ndose en el despliegue de modelos y buenas prÃ¡cticas de CI/CD.

2. ğŸ§© Estructura del Proyecto
â”œâ”€â”€ app.py                # CÃ³digo principal de la API Flask
â”œâ”€â”€ Dockerfile            # ConfiguraciÃ³n para contenedor Docker
â”œâ”€â”€ modelo_cancer.pkl     # Modelo entrenado y serializado
â”œâ”€â”€ requirements.txt      # Dependencias exactas del proyecto
â”œâ”€â”€ test_api.py           # Script para pruebas locales
â”œâ”€â”€ .github/workflows     # ConfiguraciÃ³n CI/CD (GitHub Actions)
â””â”€â”€ train_model.py        # Entrenamiento del modelo (no producciÃ³n)

3. âš™ï¸ CI/CD con GitHub y Render

El proyecto utiliza GitHub Actions para integraciÃ³n continua (CI/CD).
Cada push al repositorio desencadena un deploy automÃ¡tico en Render, reconstruyendo la imagen Docker y desplegando la API.

ğŸ”’ Se fijaron versiones especÃ­ficas de librerÃ­as para evitar incompatibilidades al cargar el modelo.

4. ğŸ§ª Pruebas de PredicciÃ³n
âœ… PredicciÃ³n Benigna (0)
curl -X POST https://evaluacion-modular-flask-cancer-api3.onrender.com/predict \
-H "Content-Type: application/json" \
-d '{"features": [20.29, 14.34, 135.1, 1297.0, 0.1003, 0.1328, 0.198, 0.1043, 0.1809, 0.0588, 0.7572, 0.7813, 5.438, 94.44, 0.01149, 0.02461, 0.05688, 0.01885, 0.01756, 0.005115, 22.54, 16.67, 152.2, 1575.0, 0.1374, 0.205, 0.4, 0.1625, 0.2364, 0.07678]}'

âš ï¸ PredicciÃ³n Maligna (1)
curl -X POST https://evaluacion-modular-flask-cancer-api3.onrender.com/predict \
-H "Content-Type: application/json" \
-d '{"features": [13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766, 0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023, 15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259]}'

5. ğŸ§¾ ConclusiÃ³n

El proyecto fue desplegado exitosamente en Render.com, con una API funcional, estable y versionada.
Se demostrÃ³ su correcto funcionamiento mediante pruebas directas desde lÃ­nea de comandos (curl y test_api.py).

6. â˜ï¸ JustificaciÃ³n del Entorno de Despliegue y Monitoreo

Aunque la consigna sugerÃ­a AWS, Azure o GCP, se eligiÃ³ Render.com por su:

Simplicidad e integraciÃ³n nativa con GitHub.

Despliegue Ã¡gil de APIs Flask sin necesidad de VMs ni buckets.

Monitoreo bÃ¡sico de logs desde el dashboard.

ğŸ§­ Ideal para entornos acadÃ©micos o prototipos donde la agilidad prima sobre la infraestructura compleja.

7. ğŸ“Š Monitoreo BÃ¡sico del Endpoint

Desde el dashboard de Render.com se visualizan:

Logs del contenedor en tiempo real.

Estado del despliegue (Ã©xito/error).

Historial de deploys.

Se realizaron pruebas manuales para medir latencia y estabilidad.
Para producciÃ³n se recomienda integrar herramientas como Prometheus + Grafana o UptimeRobot.

8. ğŸ“ˆ MÃ©tricas de Uso y DesempeÃ±o (Bonus)
â± Latencia

Medida con PowerShell:

Measure-Command {
  Invoke-WebRequest -Uri "https://evaluacion-modular-flask-cancer-api3.onrender.com/predict" `
  -Method POST `
  -Body '{"features": [14.2, 20.0, 92.0, 640.0, 0.1, 0.15, 0.2, 0.08, 0.2, 0.06, 0.6, 1.0, 4.5, 50.0, 0.007, 0.025, 0.03, 0.01, 0.02, 0.004, 16.0, 27.0, 115.0, 800.0, 0.14, 0.3, 0.35, 0.12, 0.25, 0.08]}' `
  -ContentType "application/json"
}


Resultados: entre 0.3 y 0.8 segundos por solicitud.

ğŸ” Frecuencia de uso

Durante la validaciÃ³n se realizaron mÃ¡s de 10 peticiones al endpoint /predict.
Se sugiere integrar herramientas como Postman Monitor, Google Analytics API Gateway o UptimeRobot para seguimiento continuo.

9. ğŸ–¼ Capturas Clave
SecciÃ³n	DescripciÃ³n
ğŸ“ Estructura del Proyecto	Vista general de archivos y carpetas
ğŸ§· Repositorio GitHub	CÃ³digo fuente con CI/CD
ğŸš€ Deploy en Render	Logs y estado de despliegue
ğŸ§ª Pruebas API	Curl o test_api.py mostrando {"prediction": 0} o 1
ğŸ“Š Monitoreo y MÃ©tricas	Logs y latencias observadas
âš ï¸ Error Handling (opcional)	Captura de error controlado 400/415
10. ğŸ”® Mejoras Futuras y Escalabilidad

Aunque Render cumple perfectamente para este proyecto, una futura migraciÃ³n a Kubernetes permitirÃ­a:

ğŸ” Auto-scaling segÃºn demanda.

ğŸ§¬ Despliegues canary o blue-green.

âš–ï¸ Balanceo de carga entre rÃ©plicas.

ğŸ“¡ Monitoreo avanzado con Prometheus + Grafana.


ğŸ§  Flask API â€“ PredicciÃ³n de CÃ¡ncer de Mama
1. ğŸš€ IntroducciÃ³n

Este proyecto implementa una API Flask que utiliza un modelo de clasificaciÃ³n para predecir el diagnÃ³stico de cÃ¡ncer de mama a partir del dataset Breast Cancer Wisconsin disponible en scikit-learn.

El sistema fue desarrollado como parte de la evaluaciÃ³n modular del MÃ³dulo de Machine Learning, enfocÃ¡ndose en el despliegue de modelos y buenas prÃ¡cticas de CI/CD.

2. ğŸ§© Estructura del Proyecto
â”œâ”€â”€ app.py                # CÃ³digo principal de la API Flask
â”œâ”€â”€ Dockerfile            # ConfiguraciÃ³n para contenedor Docker
â”œâ”€â”€ modelo_cancer.pkl     # Modelo entrenado y serializado
â”œâ”€â”€ requirements.txt      # Dependencias exactas del proyecto
â”œâ”€â”€ test_api.py           # Script para pruebas locales
â”œâ”€â”€ .github/workflows     # ConfiguraciÃ³n CI/CD (GitHub Actions)
â””â”€â”€ train_model.py        # Entrenamiento del modelo (no producciÃ³n)

3. âš™ï¸ CI/CD con GitHub y Render

El proyecto utiliza GitHub Actions para integraciÃ³n continua (CI/CD).
Cada push al repositorio desencadena un deploy automÃ¡tico en Render, reconstruyendo la imagen Docker y desplegando la API.

ğŸ”’ Se fijaron versiones especÃ­ficas de librerÃ­as para evitar incompatibilidades al cargar el modelo.

4. ğŸ§ª Pruebas de PredicciÃ³n
âœ… PredicciÃ³n Benigna (0)
curl -X POST https://evaluacion-modular-flask-cancer-api3.onrender.com/predict \
-H "Content-Type: application/json" \
-d '{"features": [20.29, 14.34, 135.1, 1297.0, 0.1003, 0.1328, 0.198, 0.1043, 0.1809, 0.0588, 0.7572, 0.7813, 5.438, 94.44, 0.01149, 0.02461, 0.05688, 0.01885, 0.01756, 0.005115, 22.54, 16.67, 152.2, 1575.0, 0.1374, 0.205, 0.4, 0.1625, 0.2364, 0.07678]}'

âš ï¸ PredicciÃ³n Maligna (1)
curl -X POST https://evaluacion-modular-flask-cancer-api3.onrender.com/predict \
-H "Content-Type: application/json" \
-d '{"features": [13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766, 0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023, 15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259]}'

5. ğŸ§¾ ConclusiÃ³n

El proyecto fue desplegado exitosamente en Render.com, con una API funcional, estable y versionada.
Se demostrÃ³ su correcto funcionamiento mediante pruebas directas desde lÃ­nea de comandos (curl y test_api.py).

6. â˜ï¸ JustificaciÃ³n del Entorno de Despliegue y Monitoreo

Aunque la consigna sugerÃ­a AWS, Azure o GCP, se eligiÃ³ Render.com por su:

Simplicidad e integraciÃ³n nativa con GitHub.

Despliegue Ã¡gil de APIs Flask sin necesidad de VMs ni buckets.

Monitoreo bÃ¡sico de logs desde el dashboard.

ğŸ§­ Ideal para entornos acadÃ©micos o prototipos donde la agilidad prima sobre la infraestructura compleja.

7. ğŸ“Š Monitoreo BÃ¡sico del Endpoint

Desde el dashboard de Render.com se visualizan:

Logs del contenedor en tiempo real.

Estado del despliegue (Ã©xito/error).

Historial de deploys.

Se realizaron pruebas manuales para medir latencia y estabilidad.
Para producciÃ³n se recomienda integrar herramientas como Prometheus + Grafana o UptimeRobot.

8. ğŸ“ˆ MÃ©tricas de Uso y DesempeÃ±o (Bonus)
â± Latencia

Medida con PowerShell:

Measure-Command {
  Invoke-WebRequest -Uri "https://evaluacion-modular-flask-cancer-api3.onrender.com/predict" `
  -Method POST `
  -Body '{"features": [14.2, 20.0, 92.0, 640.0, 0.1, 0.15, 0.2, 0.08, 0.2, 0.06, 0.6, 1.0, 4.5, 50.0, 0.007, 0.025, 0.03, 0.01, 0.02, 0.004, 16.0, 27.0, 115.0, 800.0, 0.14, 0.3, 0.35, 0.12, 0.25, 0.08]}' `
  -ContentType "application/json"
}


Resultados: entre 0.3 y 0.8 segundos por solicitud.

ğŸ” Frecuencia de uso

Durante la validaciÃ³n se realizaron mÃ¡s de 10 peticiones al endpoint /predict.
Se sugiere integrar herramientas como Postman Monitor, Google Analytics API Gateway o UptimeRobot para seguimiento continuo.

9. ğŸ–¼ Capturas Clave
SecciÃ³n	DescripciÃ³n
ğŸ“ Estructura del Proyecto	Vista general de archivos y carpetas
ğŸ§· Repositorio GitHub	CÃ³digo fuente con CI/CD
ğŸš€ Deploy en Render	Logs y estado de despliegue
ğŸ§ª Pruebas API	Curl o test_api.py mostrando {"prediction": 0} o 1
ğŸ“Š Monitoreo y MÃ©tricas	Logs y latencias observadas
âš ï¸ Error Handling (opcional)	Captura de error controlado 400/415
10. ğŸ”® Mejoras Futuras y Escalabilidad

Aunque Render cumple perfectamente para este proyecto, una futura migraciÃ³n a Kubernetes permitirÃ­a:

ğŸ” Auto-scaling segÃºn demanda.

ğŸ§¬ Despliegues canary o blue-green.

âš–ï¸ Balanceo de carga entre rÃ©plicas.

ğŸ“¡ Monitoreo avanzado con Prometheus + Grafana.

Se recomienda GKE, AKS o EKS si se espera trÃ¡fico alto o mÃºltiples servicios relacionados.

ğŸ’¡ Desarrollado con pasiÃ³n por el aprendizaje continuo y la ingenierÃ­a de datos aplicada a la salud.


Se recomienda GKE, AKS o EKS si se espera trÃ¡fico alto o mÃºltiples servicios relacionados.

ğŸ’¡ Desarrollado con pasiÃ³n por el aprendizaje continuo y la ingenierÃ­a de datos aplicada a la salud.
